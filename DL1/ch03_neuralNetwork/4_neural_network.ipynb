{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 신경망\n",
    "* * *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 퍼셉트론으로 복잡한 함수도 표현할 수 있음\n",
    "+ 그러나, 가중치를 설정하는 작업 ( 원하는 결과를 출력하도록 가중치 값을 적절히 정하는 작업 ) 은 여전히 사람이 수동으로 해야함\n",
    "</br></br>\n",
    "+ 신경망의 중요한 성질 == 가중치 매개변수의 적절한 값을 데이터로부터 잗ㅇ으로 학습하는 능력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 3층 신경망 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 3층 신경망에서 수행되는 입력 ~ 출력까지의 처리 ( 순방향 처리 ) 를 구현함\n",
    "\n",
    "</br>&nbsp;</br>\n",
    "3층 신경망  \n",
    "<img src=\"../img/neural_network_3floor.png\" width='600'>\n",
    "\n",
    "+ 입력층 ( 0층 ) 뉴런 2개\n",
    "+ 첫 번째 은닉층 ( 1층 ) 뉴런 3개\n",
    "+ 두 번째 은닉층 ( 2층 ) 뉴런 2개\n",
    "+ 출력층 ( 3층 ) 뉴런 2개"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <br><br><span style=\"color: mediumaquamarine;\">**표기법 설명**</spen>\n",
    "+ 신경망에서의 계산은 행렬 계산으로 정리할 수 있다.\n",
    "+ 신경망 각 층의 계산은 행렬의 곱으로 처리할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "중요한 표기  \n",
    "<img src=\"../img/neural_network_matrix2.png\" width='600'>\n",
    "\n",
    "+ $^{(1)}$ == 1층의 가중치, 1층의 뉴런임을 뜻하는 번호\n",
    "+ $_{1 2}$ == 다음 층 뉴런 && 앞 층 뉴런"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <br><br><span style=\"color: mediumaquamarine;\">**각 층의 신호 전달 구현하기**</spen>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/neural_network_matrix3.png\" width='600'>\n",
    "\n",
    "$a^{(1)}_1 = w^{(1)}_{11}x_1 + w^{(1)}_{12}x_2 + b^{(1)}_1$\n",
    "+ 행렬의 곱을 이용하면 1층의 '가중치 부분'을 아래와같이 간소화 할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$A^{(1)} = XW^{(1)} + B^{(1)}$\n",
    "\n",
    "$$\n",
    "A^{(1)} = (a^{(1)}_1 a^{(1)}_2 a^{(1)}_3), \\quad X = (x_1 x_2), \\quad B^{(1)} = (b^{(1)}_1 b^{(1)}_2 b^{(1)}_3), \\quad \n",
    "W^{(1)} = \\begin{pmatrix}\n",
    "w^{(1)}_{11} w^{(1)}_{21} w^{(1)}_{31}\\\\\\\\\n",
    "w^{(1)}_{12} w^{(1)}_{22} w^{(1)}_{32}\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n",
      "(2,)\n",
      "(3,)\n",
      "[0.3 0.7 1.1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([1.0, 0.5])\n",
    "W1 = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])\n",
    "B1 = np.array([0.1, 0.2, 0.3])\n",
    "\n",
    "print(W1.shape)\n",
    "print(X.shape)\n",
    "print(B1.shape)\n",
    "\n",
    "A1 = np.dot(X, W1) + B1\n",
    "print(A1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "입력층에서 1층으로 신호 전달  \n",
    "<img src=\"../img/neural_network_matrix4.png\" width='600'>\n",
    "\n",
    "+ 은닉층에서 가중치 합 ( 가중 신호와 편향의 총합 ) 을 $a$로 표기하고 활성화 함수 $h()$로 변환된 신호를 $z$로 표기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3 0.7 1.1]\n",
      "[0.57444252 0.66818777 0.75026011]\n"
     ]
    }
   ],
   "source": [
    "from fucntion import sigmoid\n",
    "\n",
    "Z1 = sigmoid(A1)\n",
    "\n",
    "print(A1)\n",
    "print(Z1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1층에서 2층으로의 신호 전달  \n",
    "<img src=\"../img/neural_network_matrix5.png\" width='600'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n",
      "(3, 2)\n",
      "(2,)\n",
      "[0.51615984 1.21402696]\n",
      "[0.62624937 0.7710107 ]\n"
     ]
    }
   ],
   "source": [
    "W2 = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])\n",
    "B2 = np.array([0.1, 0.2])\n",
    "\n",
    "print(Z1.shape)\n",
    "print(W2.shape)\n",
    "print(B2.shape)\n",
    "\n",
    "A2 = np.dot(Z1, W2) + B2 # 1층의 출력이 2층의 입력이 됨 ( Z1 )\n",
    "Z2 = sigmoid(A2)\n",
    "\n",
    "print(A2)\n",
    "print(Z2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2층에서 출력층으로 신호 전달  \n",
    "<img src=\"../img/neural_network_matrix6.png\" width='600'>\n",
    "\n",
    "+ 활성화 함수만 은닉층과 다름\n",
    "+ 출력층의 활성화 함수를 $\\sigma()$로 표시하여 은닉층의 활성화 함수 $h()$와 다름을 명시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.31682708 0.69627909]\n",
      "[0.31682708 0.69627909]\n"
     ]
    }
   ],
   "source": [
    "# 항등 함수\n",
    "def identity_function(x):\n",
    "    return x\n",
    "\n",
    "W3 = np.array([[0.1, 0.3], [0.2, 0.4]])\n",
    "B3 = np.array([0.1, 0.2])\n",
    "\n",
    "A3 = np.dot(Z2, W3) + B3\n",
    "Y = identity_function(A3) # || Y = A3\n",
    "\n",
    "print(A3)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 회귀 == 항등함수\n",
    "+ 2클래스 분류 == 시그모이드 함수\n",
    "+ 다중 클래스 분류 == 소프트맥스 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <br><br><span style=\"color: mediumaquamarine;\">**구현 정리**</spen>\n",
    "+ <spen style=\"color: palevioletred;\">**신경망 관례**</spen>\n",
    "    + 가중치 == 대문자\n",
    "    + 그 외 편향과 중간 결과 등 == 소문자\n",
    "\n",
    "3층 신경망 구현 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.31682708 0.69627909]\n"
     ]
    }
   ],
   "source": [
    "def init_network():\n",
    "    network = {}\n",
    "    network['W1'] = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])\n",
    "    network['b1'] = np.array([0.1, 0.2, 0.3])\n",
    "    network['W2'] = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])\n",
    "    network['b2'] = np.array([0.1, 0.2])\n",
    "    network['W3'] = np.array([[0.1, 0.3], [0.2, 0.4]])\n",
    "    network['b3'] = np.array([0.1, 0.2])\n",
    "\n",
    "    return network\n",
    "\n",
    "def forward(network, x):\n",
    "    W1, W2, W3 = network['W1'], network['W2'], network['W3']\n",
    "    b1, b2, b3 = network['b1'], network['b2'], network['b3']\n",
    "\n",
    "    a1 = np.dot(x, W1) + b1\n",
    "    z1 = sigmoid(a1)\n",
    "    a2 = np.dot(z1, W2) + b2\n",
    "    z2 = sigmoid(a2)\n",
    "    a3 = np.dot(z2, W3) + b2\n",
    "    y = identity_function(a3)\n",
    "\n",
    "    return y\n",
    "network = init_network()\n",
    "x = np.array([1.0, 0.5])\n",
    "y = forward(network, x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ ```init_network()``` 함수 == 가중치와 편향을 초기화하고 이들을 딕셔너리 변수인 ```network```에 저장\n",
    "+ ```forward()``` 함수 == 입력 신호를 출력으로 변환하는 처리 과정을 모두 구현\n",
    "    + 신경망의 순방향 ( 입력에서 출력 방향) 구현 ← 순전파"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "school",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
