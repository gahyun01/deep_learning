{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 신경망 학습\n",
    "\n",
    "* * *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 손실함수\n",
    "+ 신경망 학습에서는 현재의 상태를 '하나의 지표'로 표현\n",
    "+ 지표를 가장 좋게 만들어주는 가중치 매개변수의 값을 탐색하는 것\n",
    "+ <spen style=\"color: palevioletred;\">**신경망은 '하나의 지표'를 기준으로 최적의 매개변수 값을 탐색**</spen>\n",
    "    + '행복지표'를 가진 사람이 그 지표를 근거로 '최적의 인생'을 탐색하듯\n",
    "+ <spen style=\"color: gold;\">**손실함수 ( loss function )**</spen> == 신경망 학습에서 사용하는 지표\n",
    "    + 손실함수는 임의의 함수를 사용할 수도 있지만 일반적으로 **오차제곱합**과 **교차 엔트로피 오차**를 사용함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 손실함수는 신경망 성능의 <spen style=\"color: rosybrown;\">**'나쁨'**</spen>을 나타내는 지표\n",
    "    + 현재의 신경망이 훈련 데이터를 얼마나 잘 처리하느나 <spen style=\"color: rosybrown;\">**'못'**</spen>하느냐를 나타냄\n",
    "    + 손실함수 * 마이너스 == <spen style=\"color: rosybrown;\">**'얼마나 나쁘지 않나'**</spen> 지표 == <spen style=\"color: rosybrown;\">**'얼마나 좋으냐'**</spen> 지표\n",
    "+ <spen style=\"color: palevioletred;\">**성능의 '나쁨'과 '좋음' 중 어느쪽을 지표로 삼아도 본질적으로 수행하는 일은 다르지 않음**</spen>\n",
    "    + '나쁨을 최소로 하는 것'과 '좋음을 최대로 하는 것'은 결국 같음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <br><br><span style=\"color: mediumaquamarine;\">**오차제곱합 ( SSE : Sum of Squares for Error )**</spen>\n",
    "+ 가장 많이 쓰이는 손실 함수\n",
    "\n",
    "$$\n",
    "E = \\frac{1}{2}\\sum_{k} (y_k - t_k)^2\n",
    "$$\n",
    "+ $y_k$ == 신경망의 출력 ( 신경망이 추정한 값 )\n",
    "    + 소프트맥스 함수의 출력 → 해당 인덱스일 확률\n",
    "+ $t_k$ == 정답 레이블\n",
    "    + 원-핫 인코딩 ( 정답에 해당하는 인덱스의 원소만 1이고 나머지 0 )\n",
    "+ $k$ == 데이터의 차원 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.6 손글씨 숫자 인식\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0] # 예측값\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0] # 정답값"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 신경망의 출력 ```y``` == 소프트맥스 함수의 출력 ( 소프트맥스 함수의 출력 == 확률 )\n",
    "    + 이미지가 '0'일 확률 == 0.1\n",
    "    + 이미지가 '1'일 확률 == 0.05\n",
    "    + 이미지가 '2'일 확률 == 0.6\n",
    "+ ```t``` 의 원소의 값 == 1 → 정답 == '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 오차제곱합\n",
    "def sum_sqares_error(y, t):\n",
    "    return 0.5 * np.sum((y - t) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09750000000000003\n",
      "0.5975\n"
     ]
    }
   ],
   "source": [
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0] # 정답 == '2'\n",
    "\n",
    "# '2'일 확률이 가장 높다고 추정 ( 0.6 )\n",
    "y1 = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "\n",
    "print(sum_sqares_error(np.array(y1), np.array(t)))\n",
    "\n",
    "# '7'일 확률이 가장 높다고 추정 ( 0.6 )\n",
    "y2 = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "\n",
    "print(sum_sqares_error(np.array(y2), np.array(t)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ ```y1``` == 신경망의 출력 '2'\n",
    "+ ```y2``` == 신경망의 출력 '7'\n",
    "\n",
    "</br></br>\n",
    "<spen style=\"color: palevioletred;\">**오차 제곱합 기준으로 ```y1```의 추정 결과가 ( 오차가 더 작으니 ) 정답에 더 가까울 것으로 판단할 수 있음**</spen>  \n",
    "+ 실험 결과 == ```y1```의 손실 함수 쪽 출력이 작으며 정답 레이블과의 오차도 작음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <br><br><span style=\"color: mediumaquamarine;\">**교차 엔트로피 오차 ( CEE : Cross Entropy Error )**</spen>\n",
    "\n",
    "$$\n",
    "E = -\\sum_k t_k \\log y_k\n",
    "$$\n",
    "\n",
    "+ $log$ == 밑이 $e$인 자연로그 ( $\\log_e$ )\n",
    "+ $y_k$ == 신경망의 출력 ( 신경망이 추정한 값 )\n",
    "    + 소프트맥스 함수의 출력 → 해당 인덱스일 확률\n",
    "+ $t_k$ == 정답 레이블\n",
    "    + 원-핫 인코딩 ( 정답에 해당하는 인덱스의 원소만 1이고 나머지 0 )\n",
    "+ $k$ == 데이터의 차원 수\n",
    "\n",
    "<spen style=\"color: palevioletred;\">**교차 엔트로피 오차는 정답일 때의 출력이 전체 값을 정함**</spen>  \n",
    "+ 신경망 출력 0.6 : $-\\log 0.6 = 0.51$\n",
    "+ 신경망 출력 0.1 : $-\\log 0.1 = 2.30$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자연로그 $y = \\log x$의 그래프  \n",
    "\n",
    "<img src=\"../img/y=logx.png\" width='600'>\n",
    "\n",
    "+ 자연로그 그래프 : $x = 1$ 일때 $y = 0$, $x$가 0에 가까워질수록 $y$의 값이 점점 작아짐\n",
    "</br></br>\n",
    "+ 교차 엔트로피 오차 : 정답에 해당하는 출력이 커질수록 0에 다가가다가, 출력이 1일 때 0이 됨\n",
    "+ 반대로 정답일 때의 출력이 작아질수록 오차는 커짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 교차 엔트로피 오차\n",
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t * np.log(y + delta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ ```np.log()```를 계산할 때 아주 작은 값인 ```delta```를 더함\n",
    "    + ```np.log()``` 함수에 0을 입력하면 ```-inf``` ( 마이너스 무한대 ) 가 되어 더 이상 계산을 진행할 수 없음\n",
    "    + 마이너스 무한대가 발생하지 않도록 ( 절대 0이 되지 않도록 ) 아주 작은 값인 ```delta```를 더함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.510825457099338\n",
      "2.302584092994546\n"
     ]
    }
   ],
   "source": [
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0] # 정답 == '2'\n",
    "\n",
    "# '2'일 확률이 가장 높다고 추정 ( 0.6 )\n",
    "y1 = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "\n",
    "print(cross_entropy_error(np.array(y1), np.array(t)))\n",
    "\n",
    "# '7'일 확률이 가장 높다고 추정 ( 0.6 )\n",
    "y2 = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "\n",
    "print(cross_entropy_error(np.array(y2), np.array(t)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ ```y1``` == 출력이 0.6일 때 ( 교차 엔트로피 오차 $\\approx 0.51$ )\n",
    "+ ```y1``` == 출력이 0.1일 때 ( 교차 엔트로피 오차 $\\approx 2.3$ ) ← 더 낮음\n",
    "+ <spen style=\"color: palevioletred;\">**결과 ( 오차값 ) 가 더 작은 첫 번째 추정이 정답일 가능성이 높음**</spen> ( 오차제곱합의 판단과 일치 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <br><br><span style=\"color: mediumaquamarine;\">**미니배치 학습**</spen>\n",
    "+ 기계학습 문제는 훈련 데이터를 사용해 학습\n",
    "    + <spen style=\"color: palevioletred;\">**훈련 데이터에 대한 손실 함수의 값을 구하고, 그 값을 최대한 줄여주는 매개변수를 찾아냄**</spen>\n",
    "        + 모든 훈련 데이터를 대상으로 손살 함수 값을 구해야함 ( 훈련 데이터가 100개 있으면 그로부터 계산한 100개의 손실 함수 값들의 합을 지표로 삼는 것 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<spen style=\"color: rosybrown;\">**훈련 데이터 모두에 대한 손실 함수의 합 구하는 방법**</spen>  \n",
    "+ <spen style=\"color: gold;\">**교차 엔트로피 오차**</spen>\n",
    "\n",
    "    $$\n",
    "    E = -\\frac{1}{N}\\sum_n \\sum_k t_{nk} \\log y_{nk}\n",
    "    $$\n",
    "    + $log$ == 밑이 $e$인 자연로그 ( $\\log_e$ )\n",
    "    + $y_k$ == 신경망의 출력 ( 신경망이 추정한 값 )\n",
    "        + 소프트맥스 함수의 출력 → 해당 인덱스일 확률\n",
    "    + $t_k$ == 정답 레이블\n",
    "        + 원-핫 인코딩 ( 정답에 해당하는 인덱스의 원소만 1이고 나머지 0 )\n",
    "    + $k$ == 데이터의 차원 수\n",
    "    + $N$ == 데이터의 갯수\n",
    "\n",
    "\n",
    "    <spen style=\"color: palevioletred;\">**$N$으로 나눔으로써 '평균 손실 함수'를 구함**</spen>\n",
    "    + 평균을 구해 사용하면 훈련 데이터 개수와 관계없이 언제든 통일된 지표를 얻을 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 많은 데이터를 대상으로 일일이 손실 함수를 계산하는 것은 현실적이지 않음\n",
    "    + 데이터 일부를 추려 전체의 <spen style=\"color: rosybrown;\">**'근사치'**</spen>로 이용\n",
    "    + 신경망 학습에서도 훈련 데이터로부터 일부 ( <spen style=\"color: gold;\">**미니배치 ( mini-batch )**</spen>) 만 골라 학습 수행\n",
    "        + 무작위로 $n$장을 뽑아 $n$장만을 사용하여 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "print(x_train.shape) # ( 훈련 데이터, 입력 데이터 )\n",
    "print(t_train.shape) # ( 훈련 데이터, 정답 레이블 줄 수 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ ```np.random.choice()``` 함수 == 무작위로 $n$개를 뽑아냄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = x_train.shape[0]\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size, batch_size)\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([55798, 57024, 18399, 37286, 37386, 31817,  5064, 48508,  7021,\n",
       "        4726])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(60000, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <br><br><span style=\"color: mediumaquamarine;\">**( 배치용 ) 교차 엔트로피 오차 구현하기**</spen>\n",
    "+ 데이터가 하나인 경우와 데이터가 배치로 묶여 입력될 경우 모두를 처리할 수 있도록 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ( 배치용 ) 교차 엔트로피 오차\n",
    "def cross_entropy_error(y, t):\n",
    "    # 1차원인 경우\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = t.reshape(1, y.size)\n",
    "\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t * np.log(y + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ ```y``` == 신경망의 출력\n",
    "+ ```t``` == 정답 레이블\n",
    "\n",
    "+ ```y```가 1차원이라면, 즉 데이터 하나당 교차 엔트로피 오차를 구하는 경우는 ```reshape```함수로 데이터의 형상을 바꿔줌\n",
    "+ 배치의 크기로 나눠 정규화하고 이미지 1장당 평균의 교차 엔트로피 오차를 계산함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ( 배치용 ) 교차 엔트로피 오차 < 정답 레이블이 정수인 경우 >\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ <spen style=\"color: palevioletred;\">**핵심 == 원-핫 인코딩일 때 t가 0인 원소는 교차 엔트로피 오차도 0이므로, 그 계산은 무시해도 좋다.**</spen>\n",
    "    + 정답에 해당하는 신경망의 출력만으로 교차 엔트로피 오차를 계산할 수 있음\n",
    "+ 원-핫 인코딩 시 ```t * np.log(y)```였던 부분을 레이블 표현일 때는 ```np.log(y[np.arange(batch_size), t])```로 구현\n",
    "    + ```np.arange(batch_size)``` == 0 ~ batch_size - 1 까지 배열을 생성\n",
    "    + ```np.log(y[np.arange(batch_size), t])``` == 각 행마다 레이블에 해당하는 확률"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <br><br><span style=\"color: mediumaquamarine;\">**왜 손실 함수를 설정하는가?**</spen>\n",
    "+ 숫자 인식의 경우도 궁극적인 목적 == 높은 <spen style=\"color: rosybrown;\">**'정확도'**</spen>를 끌어내는 매개변수 값을 찾는 것\n",
    "+ <spen style=\"color: rosybrown;\">**'정확도'**</spen>라는 지표를 놔두고 <spen style=\"color: rosybrown;\">**'손실 함수의 값'**</spen>이라는 우회적인 방법을 택하는 이유\n",
    "    + <spen style=\"color: rosybrown;\">**신경망 학습에서의 '미분'의 역할**</spen>에 주목하면 알 수 있음\n",
    "        + 신경망 학습에서는 최적의 매개변수 ( 가중치와 편향 ) 를 탐색할 때 손실 함수의 값을 가능한 한 작게 하는 매개변수 값을 찾음\n",
    "        + 이때 매개변수의 미분 ( 정확히는 기울기 ) 를 계산하고, 그 미분 값을 단서로 매개변수의 값을 서서히 갱신하는 과정을 반복함\n",
    "    + <spen style=\"color: palevioletred;\">**가중치 매개변수의 손실 함수의 미분 == '가중치 매개변수의 값을 아주 조금 변화시켰을 때, 손실 함수가 어떻게 변하나'**</spen>\n",
    "        + 미분값 <spen style=\"color: rosybrown;\">**음수**</spen> == <spen style=\"color: rosybrown;\">**가중치 매개변수를 양의 방향**</spen>으로 변화시켜 손실함수의 값을 줄일 수 있음\n",
    "        + 미분값 <spen style=\"color: rosybrown;\">**양수**</spen> == <spen style=\"color: rosybrown;\">**가중치 매개변수를 음의 방향**</spen>으로 변화시겨 손실함수의 값을 줄일 수 있음\n",
    "        + 미분값 <spen style=\"color: rosybrown;\">**0**</spen> == 가중치 매개변수를 어느 쪽으로 움직여도 손실 함수의 값은 줄어들지 않음\n",
    "            + 가중치 매개변수의 갱신이 멈춤\n",
    "+ <spen style=\"color: rosybrown;\">**'정확도'**</spen>를 지표로 삼아서는 안 되는 이유\n",
    "    + 미분 값이 대부분의 장소에서 0이 되어 매개변수를 갱신할 수 없기 때문\n",
    "\n",
    "        + <spen style=\"color: rosybrown;\">**'정확도'**</spen>를 지표로 삼으면 매개변수의 미분이 대부분의 장소에서 0이 되는 이유\n",
    "            + 매개변수를 약간만 조정해서는 정확도가 개선되지 않고 일정하게 유지됨\n",
    "            + 정확도가 개선된다 하더라도 그 값은 연속적인 변화보다는 불연속적인 띄엄띄엄한 값으로 바뀜\n",
    "        + <spen style=\"color: rosybrown;\">**'손실 함수'**</spen>를 지표로 삼는다면\n",
    "            + 매개변수의 값이 조금 변하면 그에 반응하여 손실 함수의 값고 연속적으로 변화함  \n",
    "\n",
    "    \n",
    "<spen style=\"color: palevioletred;\">**손실 함수의 값은 연속적이고, 변화했을 때도 값이 연속적이지만 정확도는 연속적이지 않고 불연속적인 띄엄띄엄한 값으로 바뀐다.**</spen>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 정확도는 매개변수의 미세한 변화에는 거의 반응을 보이지 않고, 반응이 있더라도 그 값이 불연속적으로 갑자기 변화함\n",
    "    + '계단 함수'를 활성화 함수로 사용하지 않는 이유\n",
    "        + 계단함수의 미분 == 대부분의 장소 ( 0이외의 곳)에서 0\n",
    "        + <spen style=\"color: rosybrown;\">**계단 함수를 이용하면 손실 함수를 지표로 삼는게 아무 의미가 없게됨**</spen>\n",
    "            + 매개변수의 작은 변화가 주는 파장을 계단 함수가 말살하여 손실 함수의 값에는 아무런 변화가 나타나지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "계단 함수와 시그모이드 함수 : 계단 함수는 대부분의 장소에서 기울기가 0이지만, 시그모이드 함수의 기울기 ( 접선 ) 는 0이 아니다.  \n",
    "\n",
    "<img src=\"../img/step_function.png\" width='800'>\n",
    "\n",
    "+ 계단 함수는 한순간만 변화를 일으키지만, 시그모이드 함수의 미분 ( 접선 ) 은 출력 ( 세로축의 값 ) 이 연속적으로 변하고 곡선의 기울기도 연속적으로 변함\n",
    "    + 시그모이드 함수의 미분은 어느 장소라도 0이 되지 않음 → 신경망에서 중요한 성질 !\n",
    "\n",
    "    \n",
    "<spen style=\"color: palevioletred;\">**시그모이드 함수의 기울기가 0이 되지 않는 덕분에 신경망이 올바르게 학습할 수 있음**</spen>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "school",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
